{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis and topic modeling of Tweets regarding vaccines in late November, early December 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Importing data, creating data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import initial libraries \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json file (converted to csv) of tweets after duplicates have been removed\n",
    "#deduplication reduced data set from 472100 tweets to 192411 tweets\n",
    "df = pd.read_csv(\"/Users/mymac/desktop/vaccines_twitter_analysis/vaccine_data/combined_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 192411 entries, 0 to 192410\n",
      "Data columns (total 37 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   id                            192411 non-null  int64  \n",
      " 1   tweet_url                     192411 non-null  object \n",
      " 2   created_at                    192411 non-null  object \n",
      " 3   parsed_created_at             192411 non-null  object \n",
      " 4   user_screen_name              192411 non-null  object \n",
      " 5   text                          192411 non-null  object \n",
      " 6   tweet_type                    192411 non-null  object \n",
      " 7   coordinates                   106 non-null     object \n",
      " 8   hashtags                      25206 non-null   object \n",
      " 9   media                         16097 non-null   object \n",
      " 10  urls                          67731 non-null   object \n",
      " 11  favorite_count                192411 non-null  int64  \n",
      " 12  in_reply_to_screen_name       106905 non-null  object \n",
      " 13  in_reply_to_status_id         103299 non-null  float64\n",
      " 14  in_reply_to_user_id           106914 non-null  float64\n",
      " 15  lang                          192411 non-null  object \n",
      " 16  place                         3777 non-null    object \n",
      " 17  possibly_sensitive            76813 non-null   object \n",
      " 18  retweet_count                 192411 non-null  int64  \n",
      " 19  retweet_or_quote_id           16604 non-null   float64\n",
      " 20  retweet_or_quote_screen_name  16604 non-null   object \n",
      " 21  retweet_or_quote_user_id      16604 non-null   float64\n",
      " 22  source                        192326 non-null  object \n",
      " 23  user_id                       192411 non-null  int64  \n",
      " 24  user_created_at               192411 non-null  object \n",
      " 25  user_default_profile_image    192411 non-null  bool   \n",
      " 26  user_description              162181 non-null  object \n",
      " 27  user_favourites_count         192411 non-null  int64  \n",
      " 28  user_followers_count          192411 non-null  int64  \n",
      " 29  user_friends_count            192411 non-null  int64  \n",
      " 30  user_listed_count             192411 non-null  int64  \n",
      " 31  user_location                 128992 non-null  object \n",
      " 32  user_name                     192404 non-null  object \n",
      " 33  user_statuses_count           192411 non-null  int64  \n",
      " 34  user_time_zone                0 non-null       float64\n",
      " 35  user_urls                     59324 non-null   object \n",
      " 36  user_verified                 192411 non-null  bool   \n",
      "dtypes: bool(2), float64(5), int64(9), object(21)\n",
      "memory usage: 51.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#get basic info about dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'tweet_url', 'created_at', 'parsed_created_at',\n",
       "       'user_screen_name', 'text', 'tweet_type', 'coordinates', 'hashtags',\n",
       "       'media', 'urls', 'favorite_count', 'in_reply_to_screen_name',\n",
       "       'in_reply_to_status_id', 'in_reply_to_user_id', 'lang', 'place',\n",
       "       'possibly_sensitive', 'retweet_count', 'retweet_or_quote_id',\n",
       "       'retweet_or_quote_screen_name', 'retweet_or_quote_user_id', 'source',\n",
       "       'user_id', 'user_created_at', 'user_default_profile_image',\n",
       "       'user_description', 'user_favourites_count', 'user_followers_count',\n",
       "       'user_friends_count', 'user_listed_count', 'user_location', 'user_name',\n",
       "       'user_statuses_count', 'user_time_zone', 'user_urls', 'user_verified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore: what are the columns?\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 ['en' 'fr' 'ml' 'und' 'it' 'et' 'mr' 'hi' 'da' 'in' 'ja' 'kn' 'tl' 'pl'\n",
      " 'es' 'th' 'ca' 'sv' 'el' 'ar' 'te' 'zh' 'pt' 'ne' 'gu' 'nl' 'or' 'lt'\n",
      " 'ru' 'de' 'ko' 'tr' 'ta' 'pa' 'no' 'cs' 'hu' 'vi' 'fi' 'fa' 'ro' 'sr'\n",
      " 'ht' 'iw' 'ur' 'km' 'ka' 'bn' 'is' 'cy' 'bg' 'uk' 'sl' 'lo' 'lv' 'eu'\n",
      " 'my' 'si' 'ps']\n"
     ]
    }
   ],
   "source": [
    "#count number of different languages in data set\n",
    "count_lang = df['lang'].unique()\n",
    "print(len(count_lang), count_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179672, 37)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets are in 59 different languages. \n",
    "# I'll be working only with tweets in English\n",
    "# so, I'll drop tweets in all other languages\n",
    "df = df[df.lang == 'en']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping non-English tweets has reduced the data set to 179,672 tweets \n",
    "\n",
    "#there are 37 columns\n",
    "#now, I will get rid of unnecessary columns\n",
    "#some columns I'm not certain of needing later on or not, so I'll keep those\n",
    "df = df.drop(['tweet_url', 'created_at', 'media', 'urls','in_reply_to_screen_name',\n",
    "       'in_reply_to_status_id', 'in_reply_to_user_id', 'retweet_or_quote_id',\n",
    "       'retweet_or_quote_screen_name', 'retweet_or_quote_user_id', 'source',\n",
    "       'user_created_at', 'user_name', 'user_verified', 'user_friends_count', 'user_listed_count',\n",
    "       'user_statuses_count', 'user_default_profile_image', 'user_description',\n",
    "       'user_favourites_count', 'user_followers_count'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'parsed_created_at', 'user_screen_name', 'text', 'tweet_type',\n",
       "       'coordinates', 'hashtags', 'favorite_count', 'lang', 'place',\n",
       "       'possibly_sensitive', 'retweet_count', 'user_id', 'user_location',\n",
       "       'user_time_zone', 'user_urls'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking which columns are left\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)\n",
    "# columns reduced from 37 to 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179672"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe has been reduced to 15 columns. I'll likely drop more columns later \n",
    "\n",
    "#now, check how many of the tweets include geo-coordinates\n",
    "no_coordinates = df[df.coordinates != 'NaN']\n",
    "len(no_coordinates)\n",
    "#none of the tweets include geo-coordinates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parsed_created_at</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>place</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_time_zone</th>\n",
       "      <th>user_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1333608117242720256</td>\n",
       "      <td>2020-12-01 03:05:27+00:00</td>\n",
       "      <td>SpeakerMentors</td>\n",
       "      <td>@Hobie_SHRED I don't think so...this is the fi...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2439831350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1333608115816828928</td>\n",
       "      <td>2020-12-01 03:05:26+00:00</td>\n",
       "      <td>JoyceWhyfor</td>\n",
       "      <td>@DrMorien1 i voted for trump but he keeps talk...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3133624726</td>\n",
       "      <td>Mexico, Maine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1333608112314576897</td>\n",
       "      <td>2020-12-01 03:05:25+00:00</td>\n",
       "      <td>BrianCCox2</td>\n",
       "      <td>@ighaworth Thank goodness the Harris-Biden duo...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>741455733875716096</td>\n",
       "      <td>Texas, USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1333608107751170048</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>TheHops31</td>\n",
       "      <td>Covid-19 vaccine: Moderna applies for FDA auth...</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>379140897</td>\n",
       "      <td>New York, New York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://devilsandpinstripes.blogspot.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1333608105515438080</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>JuCamarote</td>\n",
       "      <td>@LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>168259094</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id          parsed_created_at user_screen_name  \\\n",
       "0  1333608117242720256  2020-12-01 03:05:27+00:00   SpeakerMentors   \n",
       "1  1333608115816828928  2020-12-01 03:05:26+00:00      JoyceWhyfor   \n",
       "2  1333608112314576897  2020-12-01 03:05:25+00:00       BrianCCox2   \n",
       "3  1333608107751170048  2020-12-01 03:05:24+00:00        TheHops31   \n",
       "4  1333608105515438080  2020-12-01 03:05:24+00:00       JuCamarote   \n",
       "\n",
       "                                                text tweet_type coordinates  \\\n",
       "0  @Hobie_SHRED I don't think so...this is the fi...      reply         NaN   \n",
       "1  @DrMorien1 i voted for trump but he keeps talk...      reply         NaN   \n",
       "2  @ighaworth Thank goodness the Harris-Biden duo...      reply         NaN   \n",
       "3  Covid-19 vaccine: Moderna applies for FDA auth...   original         NaN   \n",
       "4  @LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...      reply         NaN   \n",
       "\n",
       "  hashtags  favorite_count lang place possibly_sensitive  retweet_count  \\\n",
       "0      NaN               0   en   NaN                NaN              0   \n",
       "1      NaN               0   en   NaN                NaN              0   \n",
       "2      NaN               0   en   NaN                NaN              0   \n",
       "3      NaN               0   en   NaN               True              0   \n",
       "4      NaN               0   en   NaN                NaN              0   \n",
       "\n",
       "              user_id       user_location  user_time_zone  \\\n",
       "0          2439831350                 NaN             NaN   \n",
       "1          3133624726       Mexico, Maine             NaN   \n",
       "2  741455733875716096          Texas, USA             NaN   \n",
       "3           379140897  New York, New York             NaN   \n",
       "4           168259094       San Diego, CA             NaN   \n",
       "\n",
       "                                 user_urls  \n",
       "0                                      NaN  \n",
       "1                                      NaN  \n",
       "2                                      NaN  \n",
       "3  http://devilsandpinstripes.blogspot.com  \n",
       "4                                      NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before moving forward, I'd like to reduce the data set more - by removing some additional unnecessary columns, and maybe some rows\n",
    "\n",
    "#dropping more unnecessary columns\n",
    "df = df.drop(['user_urls', 'user_time_zone', 'place', 'coordinates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parsed_created_at</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192403</th>\n",
       "      <td>1332741303503712261</td>\n",
       "      <td>2020-11-28 17:41:02+00:00</td>\n",
       "      <td>globalbreaking_</td>\n",
       "      <td>Covid-19 vaccine: GPs in NI plan rollout for o...</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1239226109981601793</td>\n",
       "      <td>London•Paris•Washington D.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192406</th>\n",
       "      <td>1332741297375830018</td>\n",
       "      <td>2020-11-28 17:41:01+00:00</td>\n",
       "      <td>partyboitopher</td>\n",
       "      <td>If you're pro covid vaccine im just gonna assu...</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1035828543563784192</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192407</th>\n",
       "      <td>1332741297178816512</td>\n",
       "      <td>2020-11-28 17:41:01+00:00</td>\n",
       "      <td>1Pembswolf</td>\n",
       "      <td>December 10th for the first vaccine roll out.</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>317934037</td>\n",
       "      <td>Wales, United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192409</th>\n",
       "      <td>1332741293781495809</td>\n",
       "      <td>2020-11-28 17:41:00+00:00</td>\n",
       "      <td>sdutIdeas</td>\n",
       "      <td>I’m a doctor who had COVID-19. A vaccine will ...</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4214805732</td>\n",
       "      <td>San Diego, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192410</th>\n",
       "      <td>1332741291541663751</td>\n",
       "      <td>2020-11-28 17:40:59+00:00</td>\n",
       "      <td>ronaldtheshort</td>\n",
       "      <td>As we learn this morning that someone else we ...</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>20603640</td>\n",
       "      <td>Austin, TX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id          parsed_created_at user_screen_name  \\\n",
       "192403  1332741303503712261  2020-11-28 17:41:02+00:00  globalbreaking_   \n",
       "192406  1332741297375830018  2020-11-28 17:41:01+00:00   partyboitopher   \n",
       "192407  1332741297178816512  2020-11-28 17:41:01+00:00       1Pembswolf   \n",
       "192409  1332741293781495809  2020-11-28 17:41:00+00:00        sdutIdeas   \n",
       "192410  1332741291541663751  2020-11-28 17:40:59+00:00   ronaldtheshort   \n",
       "\n",
       "                                                     text tweet_type hashtags  \\\n",
       "192403  Covid-19 vaccine: GPs in NI plan rollout for o...   original      NaN   \n",
       "192406  If you're pro covid vaccine im just gonna assu...   original      NaN   \n",
       "192407      December 10th for the first vaccine roll out.   original      NaN   \n",
       "192409  I’m a doctor who had COVID-19. A vaccine will ...   original      NaN   \n",
       "192410  As we learn this morning that someone else we ...   original      NaN   \n",
       "\n",
       "        favorite_count lang possibly_sensitive  retweet_count  \\\n",
       "192403               2   en                NaN              0   \n",
       "192406               0   en                NaN              0   \n",
       "192407               0   en                NaN              0   \n",
       "192409               4   en              False              1   \n",
       "192410              13   en                NaN              0   \n",
       "\n",
       "                    user_id                 user_location  \n",
       "192403  1239226109981601793  London•Paris•Washington D.C.  \n",
       "192406  1035828543563784192                           NaN  \n",
       "192407            317934037         Wales, United Kingdom  \n",
       "192409           4214805732         San Diego, California  \n",
       "192410             20603640                    Austin, TX  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking start and end date times of the data \n",
    "#tweets collected (non-continuously) from 2020-11-28 17:40:59+00:00 to 2020-12-01 03:05:27+00:00\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vader analysis\n",
    "I want to start exploring this dataset using the Vader sentiment analysis tool, to see if it can give me an initial sense regarding polarity of sentiment in the texts.\n",
    "\n",
    "Vader gives each Tweet a rating between -1 and 1, with -1 indicating a negative sentiment and 1 indicating a positive sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Vader\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neg': 0.112, 'neu': 0.858, 'pos': 0.03, 'compound': -0.7096},\n",
       " array([\"@Hobie_SHRED I don't think so...this is the first time vaccines were prepared BEFORE the testing started, so they are ahead of schedule...the investment in ones that didn't work will be a big expense, but not as big as the death count without it.\"],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing Vader on a single tweet - the first tweet in my data set\n",
    "\n",
    "documents = np.array(df['text'])\n",
    "tweet1 = documents[0:1]\n",
    "tweet = tweet1\n",
    "\n",
    "def sentiment_analyzer_scores(tweet):\n",
    "    score = analyzer.polarity_scores(tweet)\n",
    "    return score, tweet\n",
    "\n",
    "sentiment_analyzer_scores(tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vader test gives the first tweet a result of -.7, which is quite negative. \n",
    "\n",
    "A read of the text reveals that the negativity is not towards vaccines themselves, but rather towards the pandemic - with a reference to the \"deah count.\" \n",
    "\n",
    "I'll look at the Vader scores of some other random tweets from the data set, to get a sense of what the sentiment scores are actually picking up on... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neg': 0.129, 'neu': 0.871, 'pos': 0.0, 'compound': -0.4782},\n",
       " array([\"@DrMorien1 i voted for trump but he keeps talking about the vaccines, and if you listen to anthony patch it's not good\"],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing Vader on another tweet \n",
    "\n",
    "tweet2 = documents[1:2]\n",
    "tweet = tweet2\n",
    "\n",
    "def sentiment_analyzer_scores(tweet):\n",
    "    score = analyzer.polarity_scores(tweet)\n",
    "    return score, tweet\n",
    "\n",
    "sentiment_analyzer_scores(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vader test of the second tweet is also negative: -0.4.\n",
    "\n",
    "In this case, a read of the text reveals a negativity towards vaccines in general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neg': 0.111, 'neu': 0.651, 'pos': 0.238, 'compound': 0.7404},\n",
       " array(['@ighaworth Thank goodness the Harris-Biden duo are so kind to offer this option vs the dangerous and ineffective vaccine that most Americans won’t be able to afford being offered by Trump. 🙄'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet3 = documents[2:3]\n",
    "tweet = tweet3\n",
    "\n",
    "def sentiment_analyzer_scores(tweet):\n",
    "    score = analyzer.polarity_scores(tweet)\n",
    "    return score, tweet\n",
    "\n",
    "sentiment_analyzer_scores(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vader score of tweet 3 is very positive at 0.7.\n",
    "\n",
    "A read of the text shows the positivity is directed at the Biden-Harris ticket, with very NEGATIVE sentiment towards Trump and any vaccine \"offered by Trump.\" I find it very interesting that Vader scored this tweet as positive - given that it expresses both positive and negative feelings very stronly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " array(['Covid-19 vaccine: Moderna applies for FDA authorization - CNN https://t.co/4lGcFv6Fsb'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet4 = documents[3:4]\n",
    "tweet = tweet4\n",
    "\n",
    "def sentiment_analyzer_scores(tweet):\n",
    "    score = analyzer.polarity_scores(tweet)\n",
    "    return score, tweet\n",
    "\n",
    "sentiment_analyzer_scores(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader scores tweet 4 as neutral - I assume because it's a retweet of a credible news source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neg': 0.073, 'neu': 0.748, 'pos': 0.179, 'compound': 0.7264},\n",
       " array(['@LusyLuck @bleedinCubBlue @Liliana22207796 @DrLeanaWen “Without any side effects “ doesn’t exist. Everything has side effects. Regulatory agencies will review the efficacy and safety data and assess and if the benefits are more significant than the risks the vaccine will be approved and the risks listed on the label/package insert.'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet5 = documents[4:5]\n",
    "tweet = tweet5\n",
    "\n",
    "def sentiment_analyzer_scores(tweet):\n",
    "    score = analyzer.polarity_scores(tweet)\n",
    "    return score, tweet\n",
    "\n",
    "sentiment_analyzer_scores(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader scores tweet 5 as very positive. \n",
    "\n",
    "A read of the text reveals that the vocabulary is quite scientific; I'm surprised this tweet's score is not closer to neutral. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neg': 0.166, 'neu': 0.834, 'pos': 0.0, 'compound': -0.8389},\n",
       " array(['Roger Howell shut down nfl until everyone person gets a vaccine. Why are you putting lives at risks . It’s not  like you don’t make a lot of money on merchandizing that can you hold you over? A fb was in tears almost because he has to make a living and worry about covid???'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet6 = documents[5:6]\n",
    "tweet = tweet6\n",
    "\n",
    "def sentiment_analyzer_scores(tweet):\n",
    "    score = analyzer.polarity_scores(tweet)\n",
    "    return score, tweet\n",
    "\n",
    "sentiment_analyzer_scores(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader score of tweet 6 is the most negative so far: -0.8.\n",
    "    \n",
    "The text expresses negativity not towards vaccines but rather towards the pandemic; similar to tweet 1, which also had a very negative score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neg': 0.092, 'neu': 0.561, 'pos': 0.347, 'compound': 0.7731},\n",
       " array(['@centralfornia @NYorNothing @VoopaOfficial Yes true, but..... no use arguing this here. To answer the OP - yes I will take the vaccine.'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet8 = documents[15:16]\n",
    "tweet = tweet8\n",
    "\n",
    "def sentiment_analyzer_scores(tweet):\n",
    "    score = analyzer.polarity_scores(tweet)\n",
    "    return score, tweet\n",
    "\n",
    "sentiment_analyzer_scores(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader score of tweet 8 is high: 0.7.\n",
    "    \n",
    "The text does indeed reveal a positive sentiment towards a vaccine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neg': 0.0, 'neu': 0.871, 'pos': 0.129, 'compound': 0.6369},\n",
       " array(['@KamalaHarris @Railli3 Jokes.... 😂😂😂. The clone came up with a vaccine and he isn’t and wasn’t even in power 🤷\\u200d♀️. Joe LIEden and neither you....will hold office for much longer. Enjoy your make believe while you can....'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet9 = documents[500:501]\n",
    "tweet = tweet9\n",
    "\n",
    "def sentiment_analyzer_scores(tweet):\n",
    "    score = analyzer.polarity_scores(tweet)\n",
    "    return score, tweet\n",
    "\n",
    "sentiment_analyzer_scores(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vader score of tweet 9 is quite positive: 0.64.\n",
    "\n",
    "Interestingly, the sentiment of the text is all over the place - n egative towards Biden-Haris, perhaps positive towars a vaccine - it's not clear even to a human reader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neg': 0.0, 'neu': 0.838, 'pos': 0.162, 'compound': 0.2732},\n",
       " array([\"@raganbeth Except with them it's going to be mandatory. You take the vaccine or they take your livelihood.\"],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet10 = documents[1000:1001]\n",
    "tweet = tweet10\n",
    "\n",
    "def sentiment_analyzer_scores(tweet):\n",
    "    score = analyzer.polarity_scores(tweet)\n",
    "    return score, tweet\n",
    "\n",
    "sentiment_analyzer_scores(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vader score of tweet 10 is somewhat positive: 0.27.\n",
    "    \n",
    "A human read of the text reveals, however, a somewhat dystopian sentiment about forced vaccinations -- not exactly positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIAL IMPRESSION OF VADER SENTIMENT ANALYSIS: The Vader analysis of unprocessed text from tweets is quite superficial and in many cases misleading.  \n",
    "\n",
    "I could process the tweets in hopes of getting a more refined output from Vader, and then grouping the positive/neutral/negative texts into clusters in order to find similarities between them. \n",
    "\n",
    "However, I think it's better to focus my efforts on another path, doing my own modeling.\n",
    "\n",
    "To conclude the Vader analysis section, I'll include a breakdown of the distribution of sentiment across my data set (below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Vader on full dataset \n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    return sentiment_analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "df['sentiment_score'] = df.text.apply(lambda x: sentiment_analysis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_id</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.796720e+05</td>\n",
       "      <td>179672.000000</td>\n",
       "      <td>179672.000000</td>\n",
       "      <td>1.796720e+05</td>\n",
       "      <td>179672.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.333256e+18</td>\n",
       "      <td>11.516998</td>\n",
       "      <td>2.006768</td>\n",
       "      <td>4.463303e+17</td>\n",
       "      <td>0.057929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.558934e+14</td>\n",
       "      <td>496.171748</td>\n",
       "      <td>83.790658</td>\n",
       "      <td>5.553048e+17</td>\n",
       "      <td>0.484655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.332741e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.240000e+02</td>\n",
       "      <td>-0.997100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.333093e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.102260e+08</td>\n",
       "      <td>-0.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.333333e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.406998e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.333469e+18</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.065440e+18</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.333608e+18</td>\n",
       "      <td>155735.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>1.333592e+18</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  favorite_count  retweet_count       user_id  \\\n",
       "count  1.796720e+05   179672.000000  179672.000000  1.796720e+05   \n",
       "mean   1.333256e+18       11.516998       2.006768  4.463303e+17   \n",
       "std    2.558934e+14      496.171748      83.790658  5.553048e+17   \n",
       "min    1.332741e+18        0.000000       0.000000  3.240000e+02   \n",
       "25%    1.333093e+18        0.000000       0.000000  2.102260e+08   \n",
       "50%    1.333333e+18        0.000000       0.000000  2.406998e+09   \n",
       "75%    1.333469e+18        2.000000       0.000000  1.065440e+18   \n",
       "max    1.333608e+18   155735.000000   24783.000000  1.333592e+18   \n",
       "\n",
       "       sentiment_score  \n",
       "count    179672.000000  \n",
       "mean          0.057929  \n",
       "std           0.484655  \n",
       "min          -0.997100  \n",
       "25%          -0.296000  \n",
       "50%           0.000000  \n",
       "75%           0.443400  \n",
       "max           0.999800  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at basic stats of the sentiment score for full dataset\n",
    "df.describe()\n",
    "#mean is on the slighly positive side at 0.05 (with zero being a neutral score)\n",
    "#std of 0.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at distribution of sentiment score \n",
    "\n",
    "df['very_negative'] = df.sentiment_score.apply(lambda x: x < -0.5)\n",
    "df['negative'] = df.sentiment_score.apply(lambda x: x >= -0.5 and x < 0)\n",
    "df['neutral'] = df.sentiment_score.apply(lambda x: x == 0)\n",
    "df['positive'] = df.sentiment_score.apply(lambda x: x > 0 and x <= 0.5)\n",
    "df['very_positive'] = df.sentiment_score.apply(lambda x: x > 0.5)\n",
    "\n",
    "count_very_negative = list(df[df.very_negative == True].count())[0]\n",
    "count_negative = list(df[df.negative == True].count())[0] \n",
    "count_neutral = list(df[df.neutral == True].count())[0]\n",
    "count_positive = list(df[df.positive == True].count())[0]\n",
    "count_very_positive = list(df[df.very_positive == True].count())[0] \n",
    "\n",
    "df['negative'] = df.sentiment_score.apply(lambda x: x >= -0.5 and x < 0)\n",
    "df['neutral'] = df.sentiment_score.apply(lambda x: x == 0)\n",
    "df['positive'] = df.sentiment_score.apply(lambda x: x > 0 and x <= 0.5)\n",
    "df['very_positive'] = df.sentiment_score.apply(lambda x: x > 0.5)\n",
    "\n",
    "sentiment_counts = [count_very_negative, count_negative, count_neutral, count_positive, count_very_positive]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28655, 32954, 37678, 42111, 38274]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_counts \n",
    "#distribution (list below of totals) is skewed to the left, as indicated by the slightly positive mean score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import seaborn as sns\n",
    "\n",
    "# # plot histogram of sentiments\n",
    "# fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "# plt.hist(\n",
    "#     sentiment_counts,\n",
    "#     bins=20,\n",
    "#     color='#60505C')\n",
    "\n",
    "# plt.title('Distribution - Article Word Count', fontsize=16)\n",
    "# plt.ylabel('Frequency', fontsize=12)\n",
    "# plt.xlabel('Word Count', fontsize=12)\n",
    "# plt.yticks(np.arange(0, 50, 5))\n",
    "# plt.xticks(np.arange(0, 2700, 200))\n",
    "\n",
    "# file_name = 'hist'\n",
    "\n",
    "# fig.savefig(\n",
    "#     file_path + file_name + '.png',\n",
    "#     dpi=fig.dpi,\n",
    "#     bbox_inches='tight'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Processing of data in preparation for text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reminder: my \"corpus\" is the numpy array called \"documents\"\n",
    "documents = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mymac/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#I imported this code from a great project on GitHub - I'm adapting it for my purposes. Credit to: \n",
    "#source: https://github.com/robsalgado/personal_data_science_projects/blob/master/topic_modeling_nmf/nlp_topic_utils.ipynb\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Contraction map\n",
    "c_dict = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"i'd've\": \"I would have\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"i'll've\": \"I will have\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'alls\": \"you alls\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you you will\",\n",
    "    \"you'll've\": \"you you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Compiling the contraction dict\n",
    "c_re = re.compile('(%s)' % '|'.join(c_dict.keys()))\n",
    "\n",
    "# List of stop words\n",
    "add_stop = ['said', 'say', '...', 'like']\n",
    "stop_words = ENGLISH_STOP_WORDS.union(add_stop)\n",
    "\n",
    "# List of punctuation\n",
    "punc = list(set(string.punctuation))\n",
    "\n",
    "\n",
    "# Splits words on white spaces (leaves contractions intact) and splits out\n",
    "# trailing punctuation\n",
    "def casual_tokenizer(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return c_dict[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    text = casual_tokenizer(text)\n",
    "    text = [each.lower() for each in text]\n",
    "    text = [re.sub('[0-9]+', '', each) for each in text]\n",
    "    text = [expandContractions(each, c_re=c_re) for each in text]\n",
    "    text = [w for w in text if w not in punc]\n",
    "    #text = [w for w in text if w not in stop_words]\n",
    "    text = [each for each in text if len(each) > 1]\n",
    "    text = [each for each in text if ' ' not in each]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process text\n",
    "df['processed_text'] = df['text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parsed_created_at</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_location</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>very_negative</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>very_positive</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1333608117242720256</td>\n",
       "      <td>2020-12-01 03:05:27+00:00</td>\n",
       "      <td>SpeakerMentors</td>\n",
       "      <td>@Hobie_SHRED I don't think so...this is the fi...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2439831350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.7096</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[@hobie_shred, think, so, ..., this, is, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1333608115816828928</td>\n",
       "      <td>2020-12-01 03:05:26+00:00</td>\n",
       "      <td>JoyceWhyfor</td>\n",
       "      <td>@DrMorien1 i voted for trump but he keeps talk...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3133624726</td>\n",
       "      <td>Mexico, Maine</td>\n",
       "      <td>-0.4782</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[@drmorien, voted, for, trump, but, he, keeps,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1333608112314576897</td>\n",
       "      <td>2020-12-01 03:05:25+00:00</td>\n",
       "      <td>BrianCCox2</td>\n",
       "      <td>@ighaworth Thank goodness the Harris-Biden duo...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>741455733875716096</td>\n",
       "      <td>Texas, USA</td>\n",
       "      <td>0.7404</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[@ighaworth, thank, goodness, the, harris-bide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1333608107751170048</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>TheHops31</td>\n",
       "      <td>Covid-19 vaccine: Moderna applies for FDA auth...</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>379140897</td>\n",
       "      <td>New York, New York</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[covid, vaccine, moderna, applies, for, fda, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1333608105515438080</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>JuCamarote</td>\n",
       "      <td>@LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>168259094</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>0.7264</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[@lusyluck, @bleedincubblue, @liliana, @drlean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id          parsed_created_at user_screen_name  \\\n",
       "0  1333608117242720256  2020-12-01 03:05:27+00:00   SpeakerMentors   \n",
       "1  1333608115816828928  2020-12-01 03:05:26+00:00      JoyceWhyfor   \n",
       "2  1333608112314576897  2020-12-01 03:05:25+00:00       BrianCCox2   \n",
       "3  1333608107751170048  2020-12-01 03:05:24+00:00        TheHops31   \n",
       "4  1333608105515438080  2020-12-01 03:05:24+00:00       JuCamarote   \n",
       "\n",
       "                                                text tweet_type hashtags  \\\n",
       "0  @Hobie_SHRED I don't think so...this is the fi...      reply      NaN   \n",
       "1  @DrMorien1 i voted for trump but he keeps talk...      reply      NaN   \n",
       "2  @ighaworth Thank goodness the Harris-Biden duo...      reply      NaN   \n",
       "3  Covid-19 vaccine: Moderna applies for FDA auth...   original      NaN   \n",
       "4  @LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...      reply      NaN   \n",
       "\n",
       "   favorite_count lang possibly_sensitive  retweet_count             user_id  \\\n",
       "0               0   en                NaN              0          2439831350   \n",
       "1               0   en                NaN              0          3133624726   \n",
       "2               0   en                NaN              0  741455733875716096   \n",
       "3               0   en               True              0           379140897   \n",
       "4               0   en                NaN              0           168259094   \n",
       "\n",
       "        user_location  sentiment_score  very_negative  negative  neutral  \\\n",
       "0                 NaN          -0.7096           True     False    False   \n",
       "1       Mexico, Maine          -0.4782          False      True    False   \n",
       "2          Texas, USA           0.7404          False     False    False   \n",
       "3  New York, New York           0.0000          False     False     True   \n",
       "4       San Diego, CA           0.7264          False     False    False   \n",
       "\n",
       "   positive  very_positive                                     processed_text  \n",
       "0     False          False  [@hobie_shred, think, so, ..., this, is, the, ...  \n",
       "1     False          False  [@drmorien, voted, for, trump, but, he, keeps,...  \n",
       "2     False           True  [@ighaworth, thank, goodness, the, harris-bide...  \n",
       "3     False          False  [covid, vaccine, moderna, applies, for, fda, a...  \n",
       "4     False           True  [@lusyluck, @bleedincubblue, @liliana, @drlean...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [@hobie_shred, think, so, ..., this, is, the, ...\n",
       "1    [@drmorien, voted, for, trump, but, he, keeps,...\n",
       "2    [@ighaworth, thank, goodness, the, harris-bide...\n",
       "Name: processed_text, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_text'][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE TO SELF: need to remove stopwords parts-of-speech tagging, following by lemmatizing\n",
    "#then processing will be complete\n",
    "\n",
    "def pos_tagging(text):\n",
    "    pos_tag = [pos_tag(word) for word in processed_text]\n",
    "    \n",
    "df['pos_tagged'] = df.processed_text.apply(lambda x: pos_tag(x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parsed_created_at</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_location</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>very_negative</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>very_positive</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>pos_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1333608117242720256</td>\n",
       "      <td>2020-12-01 03:05:27+00:00</td>\n",
       "      <td>SpeakerMentors</td>\n",
       "      <td>@Hobie_SHRED I don't think so...this is the fi...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2439831350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.7096</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[@hobie_shred, think, so, ..., this, is, the, ...</td>\n",
       "      <td>[(@hobie_shred, VBN), (think, VBP), (so, RB), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1333608115816828928</td>\n",
       "      <td>2020-12-01 03:05:26+00:00</td>\n",
       "      <td>JoyceWhyfor</td>\n",
       "      <td>@DrMorien1 i voted for trump but he keeps talk...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3133624726</td>\n",
       "      <td>Mexico, Maine</td>\n",
       "      <td>-0.4782</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[@drmorien, voted, for, trump, but, he, keeps,...</td>\n",
       "      <td>[(@drmorien, NN), (voted, VBD), (for, IN), (tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1333608112314576897</td>\n",
       "      <td>2020-12-01 03:05:25+00:00</td>\n",
       "      <td>BrianCCox2</td>\n",
       "      <td>@ighaworth Thank goodness the Harris-Biden duo...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>741455733875716096</td>\n",
       "      <td>Texas, USA</td>\n",
       "      <td>0.7404</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[@ighaworth, thank, goodness, the, harris-bide...</td>\n",
       "      <td>[(@ighaworth, JJ), (thank, NN), (goodness, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1333608107751170048</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>TheHops31</td>\n",
       "      <td>Covid-19 vaccine: Moderna applies for FDA auth...</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>379140897</td>\n",
       "      <td>New York, New York</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[covid, vaccine, moderna, applies, for, fda, a...</td>\n",
       "      <td>[(covid, JJ), (vaccine, NN), (moderna, NN), (a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1333608105515438080</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>JuCamarote</td>\n",
       "      <td>@LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>168259094</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>0.7264</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[@lusyluck, @bleedincubblue, @liliana, @drlean...</td>\n",
       "      <td>[(@lusyluck, JJ), (@bleedincubblue, NNP), (@li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id          parsed_created_at user_screen_name  \\\n",
       "0  1333608117242720256  2020-12-01 03:05:27+00:00   SpeakerMentors   \n",
       "1  1333608115816828928  2020-12-01 03:05:26+00:00      JoyceWhyfor   \n",
       "2  1333608112314576897  2020-12-01 03:05:25+00:00       BrianCCox2   \n",
       "3  1333608107751170048  2020-12-01 03:05:24+00:00        TheHops31   \n",
       "4  1333608105515438080  2020-12-01 03:05:24+00:00       JuCamarote   \n",
       "\n",
       "                                                text tweet_type hashtags  \\\n",
       "0  @Hobie_SHRED I don't think so...this is the fi...      reply      NaN   \n",
       "1  @DrMorien1 i voted for trump but he keeps talk...      reply      NaN   \n",
       "2  @ighaworth Thank goodness the Harris-Biden duo...      reply      NaN   \n",
       "3  Covid-19 vaccine: Moderna applies for FDA auth...   original      NaN   \n",
       "4  @LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...      reply      NaN   \n",
       "\n",
       "   favorite_count lang possibly_sensitive  retweet_count             user_id  \\\n",
       "0               0   en                NaN              0          2439831350   \n",
       "1               0   en                NaN              0          3133624726   \n",
       "2               0   en                NaN              0  741455733875716096   \n",
       "3               0   en               True              0           379140897   \n",
       "4               0   en                NaN              0           168259094   \n",
       "\n",
       "        user_location  sentiment_score  very_negative  negative  neutral  \\\n",
       "0                 NaN          -0.7096           True     False    False   \n",
       "1       Mexico, Maine          -0.4782          False      True    False   \n",
       "2          Texas, USA           0.7404          False     False    False   \n",
       "3  New York, New York           0.0000          False     False     True   \n",
       "4       San Diego, CA           0.7264          False     False    False   \n",
       "\n",
       "   positive  very_positive                                     processed_text  \\\n",
       "0     False          False  [@hobie_shred, think, so, ..., this, is, the, ...   \n",
       "1     False          False  [@drmorien, voted, for, trump, but, he, keeps,...   \n",
       "2     False           True  [@ighaworth, thank, goodness, the, harris-bide...   \n",
       "3     False          False  [covid, vaccine, moderna, applies, for, fda, a...   \n",
       "4     False           True  [@lusyluck, @bleedincubblue, @liliana, @drlean...   \n",
       "\n",
       "                                          pos_tagged  \n",
       "0  [(@hobie_shred, VBN), (think, VBP), (so, RB), ...  \n",
       "1  [(@drmorien, NN), (voted, VBD), (for, IN), (tr...  \n",
       "2  [(@ighaworth, JJ), (thank, NN), (goodness, NN)...  \n",
       "3  [(covid, JJ), (vaccine, NN), (moderna, NN), (a...  \n",
       "4  [(@lusyluck, JJ), (@bleedincubblue, NNP), (@li...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagged = df['pos_tagged']\n",
    "type(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [(@hobie_shred, VBN), (think, VBP), (so, RB), ...\n",
       "1         [(@drmorien, NN), (voted, VBD), (for, IN), (tr...\n",
       "2         [(@ighaworth, JJ), (thank, NN), (goodness, NN)...\n",
       "3         [(covid, JJ), (vaccine, NN), (moderna, NN), (a...\n",
       "4         [(@lusyluck, JJ), (@bleedincubblue, NNP), (@li...\n",
       "                                ...                        \n",
       "192403    [(covid, JJ), (vaccine, NN), (gps, NN), (in, I...\n",
       "192406    [(if, IN), (pro, JJ), (covid, FW), (vaccine, N...\n",
       "192407    [(december, NN), (th, NN), (for, IN), (the, DT...\n",
       "192409    [(doctor, NN), (who, WP), (had, VBD), (covid, ...\n",
       "192410    [(as, IN), (we, PRP), (learn, VBP), (this, DT)...\n",
       "Name: pos_tagged, Length: 179672, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #remove stop words\n",
    "def remove_stopwords(text):\n",
    "    text = [w for w in text if w not in stop_words]\n",
    "    return text\n",
    "\n",
    "df['pos_tagged'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['pos_tagged'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@hobie_shred', 'VBN'),\n",
       " ('think', 'VBP'),\n",
       " ('so', 'RB'),\n",
       " ('...', ':'),\n",
       " ('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('first', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " ('vaccines', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('prepared', 'VBN'),\n",
       " ('before', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('testing', 'NN'),\n",
       " ('started', 'VBD'),\n",
       " ('so', 'RB'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('ahead', 'RB'),\n",
       " ('of', 'IN'),\n",
       " ('schedule', 'NN'),\n",
       " ('...', ':'),\n",
       " ('the', 'DT'),\n",
       " ('investment', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('ones', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('work', 'VBP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('big', 'JJ'),\n",
       " ('expense', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('not', 'RB'),\n",
       " ('as', 'RB'),\n",
       " ('big', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('death', 'NN'),\n",
       " ('count', 'NN'),\n",
       " ('without', 'IN'),\n",
       " ('it', 'PRP')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pos_tagged'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wordnet = WordNetLemmatizer() \n",
    "\n",
    "lemmatized = [[wordnet.lemmatize(word[0]) for word in words] for words in pos_tagged]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179672"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@kamalaharris',\n",
       " '@railli',\n",
       " 'joke',\n",
       " '...',\n",
       " 'the',\n",
       " 'clone',\n",
       " 'came',\n",
       " 'up',\n",
       " 'with',\n",
       " 'vaccine',\n",
       " 'and',\n",
       " 'he',\n",
       " 'isn',\n",
       " 'and',\n",
       " 'wasn',\n",
       " 'even',\n",
       " 'in',\n",
       " 'power',\n",
       " 'joe',\n",
       " 'lieden',\n",
       " 'and',\n",
       " 'neither',\n",
       " 'you',\n",
       " '...',\n",
       " 'will',\n",
       " 'hold',\n",
       " 'office',\n",
       " 'for',\n",
       " 'much',\n",
       " 'longer',\n",
       " 'enjoy',\n",
       " 'your',\n",
       " 'make',\n",
       " 'believe',\n",
       " 'while',\n",
       " 'you',\n",
       " 'can',\n",
       " '...']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parsed_created_at</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_location</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>very_negative</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>very_positive</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>pos_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1333608117242720256</td>\n",
       "      <td>2020-12-01 03:05:27+00:00</td>\n",
       "      <td>SpeakerMentors</td>\n",
       "      <td>@Hobie_SHRED I don't think so...this is the fi...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2439831350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.7096</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[@hobie_shred, think, so, ..., this, is, the, ...</td>\n",
       "      <td>[(@hobie_shred, VBN), (think, VBP), (so, RB), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1333608115816828928</td>\n",
       "      <td>2020-12-01 03:05:26+00:00</td>\n",
       "      <td>JoyceWhyfor</td>\n",
       "      <td>@DrMorien1 i voted for trump but he keeps talk...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3133624726</td>\n",
       "      <td>Mexico, Maine</td>\n",
       "      <td>-0.4782</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[@drmorien, voted, for, trump, but, he, keeps,...</td>\n",
       "      <td>[(@drmorien, NN), (voted, VBD), (for, IN), (tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1333608112314576897</td>\n",
       "      <td>2020-12-01 03:05:25+00:00</td>\n",
       "      <td>BrianCCox2</td>\n",
       "      <td>@ighaworth Thank goodness the Harris-Biden duo...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>741455733875716096</td>\n",
       "      <td>Texas, USA</td>\n",
       "      <td>0.7404</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[@ighaworth, thank, goodness, the, harris-bide...</td>\n",
       "      <td>[(@ighaworth, JJ), (thank, NN), (goodness, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1333608107751170048</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>TheHops31</td>\n",
       "      <td>Covid-19 vaccine: Moderna applies for FDA auth...</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>379140897</td>\n",
       "      <td>New York, New York</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[covid, vaccine, moderna, applies, for, fda, a...</td>\n",
       "      <td>[(covid, JJ), (vaccine, NN), (moderna, NN), (a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1333608105515438080</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>JuCamarote</td>\n",
       "      <td>@LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>168259094</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>0.7264</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[@lusyluck, @bleedincubblue, @liliana, @drlean...</td>\n",
       "      <td>[(@lusyluck, JJ), (@bleedincubblue, NNP), (@li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id          parsed_created_at user_screen_name  \\\n",
       "0  1333608117242720256  2020-12-01 03:05:27+00:00   SpeakerMentors   \n",
       "1  1333608115816828928  2020-12-01 03:05:26+00:00      JoyceWhyfor   \n",
       "2  1333608112314576897  2020-12-01 03:05:25+00:00       BrianCCox2   \n",
       "3  1333608107751170048  2020-12-01 03:05:24+00:00        TheHops31   \n",
       "4  1333608105515438080  2020-12-01 03:05:24+00:00       JuCamarote   \n",
       "\n",
       "                                                text tweet_type hashtags  \\\n",
       "0  @Hobie_SHRED I don't think so...this is the fi...      reply      NaN   \n",
       "1  @DrMorien1 i voted for trump but he keeps talk...      reply      NaN   \n",
       "2  @ighaworth Thank goodness the Harris-Biden duo...      reply      NaN   \n",
       "3  Covid-19 vaccine: Moderna applies for FDA auth...   original      NaN   \n",
       "4  @LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...      reply      NaN   \n",
       "\n",
       "   favorite_count lang possibly_sensitive  retweet_count             user_id  \\\n",
       "0               0   en                NaN              0          2439831350   \n",
       "1               0   en                NaN              0          3133624726   \n",
       "2               0   en                NaN              0  741455733875716096   \n",
       "3               0   en               True              0           379140897   \n",
       "4               0   en                NaN              0           168259094   \n",
       "\n",
       "        user_location  sentiment_score  very_negative  negative  neutral  \\\n",
       "0                 NaN          -0.7096           True     False    False   \n",
       "1       Mexico, Maine          -0.4782          False      True    False   \n",
       "2          Texas, USA           0.7404          False     False    False   \n",
       "3  New York, New York           0.0000          False     False     True   \n",
       "4       San Diego, CA           0.7264          False     False    False   \n",
       "\n",
       "   positive  very_positive                                     processed_text  \\\n",
       "0     False          False  [@hobie_shred, think, so, ..., this, is, the, ...   \n",
       "1     False          False  [@drmorien, voted, for, trump, but, he, keeps,...   \n",
       "2     False           True  [@ighaworth, thank, goodness, the, harris-bide...   \n",
       "3     False          False  [covid, vaccine, moderna, applies, for, fda, a...   \n",
       "4     False           True  [@lusyluck, @bleedincubblue, @liliana, @drlean...   \n",
       "\n",
       "                                          pos_tagged  \n",
       "0  [(@hobie_shred, VBN), (think, VBP), (so, RB), ...  \n",
       "1  [(@drmorien, NN), (voted, VBD), (for, IN), (tr...  \n",
       "2  [(@ighaworth, JJ), (thank, NN), (goodness, NN)...  \n",
       "3  [(covid, JJ), (vaccine, NN), (moderna, NN), (a...  \n",
       "4  [(@lusyluck, JJ), (@bleedincubblue, NNP), (@li...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parsed_created_at</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>user_location</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>very_negative</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>very_positive</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>pos_tagged</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1333608117242720256</td>\n",
       "      <td>2020-12-01 03:05:27+00:00</td>\n",
       "      <td>SpeakerMentors</td>\n",
       "      <td>@Hobie_SHRED I don't think so...this is the fi...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.7096</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[@hobie_shred, think, so, ..., this, is, the, ...</td>\n",
       "      <td>[(@hobie_shred, VBN), (think, VBP), (so, RB), ...</td>\n",
       "      <td>[@hobie_shred, think, so, ..., this, is, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1333608115816828928</td>\n",
       "      <td>2020-12-01 03:05:26+00:00</td>\n",
       "      <td>JoyceWhyfor</td>\n",
       "      <td>@DrMorien1 i voted for trump but he keeps talk...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Mexico, Maine</td>\n",
       "      <td>-0.4782</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[@drmorien, voted, for, trump, but, he, keeps,...</td>\n",
       "      <td>[(@drmorien, NN), (voted, VBD), (for, IN), (tr...</td>\n",
       "      <td>[@drmorien, voted, for, trump, but, he, keep, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1333608112314576897</td>\n",
       "      <td>2020-12-01 03:05:25+00:00</td>\n",
       "      <td>BrianCCox2</td>\n",
       "      <td>@ighaworth Thank goodness the Harris-Biden duo...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Texas, USA</td>\n",
       "      <td>0.7404</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[@ighaworth, thank, goodness, the, harris-bide...</td>\n",
       "      <td>[(@ighaworth, JJ), (thank, NN), (goodness, NN)...</td>\n",
       "      <td>[@ighaworth, thank, goodness, the, harris-bide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1333608107751170048</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>TheHops31</td>\n",
       "      <td>Covid-19 vaccine: Moderna applies for FDA auth...</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>New York, New York</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[covid, vaccine, moderna, applies, for, fda, a...</td>\n",
       "      <td>[(covid, JJ), (vaccine, NN), (moderna, NN), (a...</td>\n",
       "      <td>[covid, vaccine, moderna, applies, for, fda, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1333608105515438080</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>JuCamarote</td>\n",
       "      <td>@LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>0.7264</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[@lusyluck, @bleedincubblue, @liliana, @drlean...</td>\n",
       "      <td>[(@lusyluck, JJ), (@bleedincubblue, NNP), (@li...</td>\n",
       "      <td>[@lusyluck, @bleedincubblue, @liliana, @drlean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id          parsed_created_at user_screen_name  \\\n",
       "0  1333608117242720256  2020-12-01 03:05:27+00:00   SpeakerMentors   \n",
       "1  1333608115816828928  2020-12-01 03:05:26+00:00      JoyceWhyfor   \n",
       "2  1333608112314576897  2020-12-01 03:05:25+00:00       BrianCCox2   \n",
       "3  1333608107751170048  2020-12-01 03:05:24+00:00        TheHops31   \n",
       "4  1333608105515438080  2020-12-01 03:05:24+00:00       JuCamarote   \n",
       "\n",
       "                                                text tweet_type hashtags  \\\n",
       "0  @Hobie_SHRED I don't think so...this is the fi...      reply      NaN   \n",
       "1  @DrMorien1 i voted for trump but he keeps talk...      reply      NaN   \n",
       "2  @ighaworth Thank goodness the Harris-Biden duo...      reply      NaN   \n",
       "3  Covid-19 vaccine: Moderna applies for FDA auth...   original      NaN   \n",
       "4  @LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...      reply      NaN   \n",
       "\n",
       "   favorite_count lang possibly_sensitive  retweet_count  ...  \\\n",
       "0               0   en                NaN              0  ...   \n",
       "1               0   en                NaN              0  ...   \n",
       "2               0   en                NaN              0  ...   \n",
       "3               0   en               True              0  ...   \n",
       "4               0   en                NaN              0  ...   \n",
       "\n",
       "        user_location sentiment_score  very_negative  negative  neutral  \\\n",
       "0                 NaN         -0.7096           True     False    False   \n",
       "1       Mexico, Maine         -0.4782          False      True    False   \n",
       "2          Texas, USA          0.7404          False     False    False   \n",
       "3  New York, New York          0.0000          False     False     True   \n",
       "4       San Diego, CA          0.7264          False     False    False   \n",
       "\n",
       "   positive  very_positive                                     processed_text  \\\n",
       "0     False          False  [@hobie_shred, think, so, ..., this, is, the, ...   \n",
       "1     False          False  [@drmorien, voted, for, trump, but, he, keeps,...   \n",
       "2     False           True  [@ighaworth, thank, goodness, the, harris-bide...   \n",
       "3     False          False  [covid, vaccine, moderna, applies, for, fda, a...   \n",
       "4     False           True  [@lusyluck, @bleedincubblue, @liliana, @drlean...   \n",
       "\n",
       "                                          pos_tagged  \\\n",
       "0  [(@hobie_shred, VBN), (think, VBP), (so, RB), ...   \n",
       "1  [(@drmorien, NN), (voted, VBD), (for, IN), (tr...   \n",
       "2  [(@ighaworth, JJ), (thank, NN), (goodness, NN)...   \n",
       "3  [(covid, JJ), (vaccine, NN), (moderna, NN), (a...   \n",
       "4  [(@lusyluck, JJ), (@bleedincubblue, NNP), (@li...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [@hobie_shred, think, so, ..., this, is, the, ...  \n",
       "1  [@drmorien, voted, for, trump, but, he, keep, ...  \n",
       "2  [@ighaworth, thank, goodness, the, harris-bide...  \n",
       "3  [covid, vaccine, moderna, applies, for, fda, a...  \n",
       "4  [@lusyluck, @bleedincubblue, @liliana, @drlean...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [@hobie_shred, think, so, ..., this, is, the, ...\n",
       "1    [@drmorien, voted, for, trump, but, he, keep, ...\n",
       "Name: lemmatized, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmatized'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: TF-IDF \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@hobie_shred think so ... this is the first time vaccine were prepared before the testing started so they are ahead of schedule ... the investment in one that work will be big expense but not a big a the death count without it'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(df['lemmatized'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_docs'] = df['lemmatized'].apply(lambda x: \" \".join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_docs = df['final_docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create document term matrix with TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# # initial tuning of parameters\n",
    "# #set max_features to 2000 (specifies the number of most frequently occurring words for which we want to create feature vectors)\n",
    "# # set min_df to 5 (word must occur in at least 5 documents)\n",
    "# # set max_df to 0.85 (word must not occur in more than 80 percent of the documents) \n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.85, ngram_range=(1, 2), stop_words='english')  \n",
    "doc_term_matrix_1 = tfidfconverter.fit_transform(df['final_docs'].values.astype('U'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_term_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179672, 2000)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: LDA modeling to identify latent topics \n",
    "\n",
    "For Parts 5 and 6, I was inspired by and adapted some code from a Github project that uses LDA and NMF modeling:\n",
    "https://stackabuse.com/python-for-nlp-topic-modeling/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=20, stop_words='english')\n",
    "doc_term_matrix_2 = count_vect.fit_transform(df['final_docs'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<179672x8803 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2047256 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LDA to create topics,  along with the probability distribution for each word in our vocabulary for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(random_state=42)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "LDA.fit(doc_term_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union\n",
      "jr\n",
      "formaldehyde\n",
      "bullying\n",
      "start\n",
      "desperately\n",
      "hurry\n",
      "unnecessary\n",
      "screaming\n",
      "sorted\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(count_vect.get_feature_names()))\n",
    "    print(count_vect.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 10 words with the highest probability for the first topic...\n",
    "# to get first topic, se the components_ attribute and pass a 0 index as the value\n",
    "first_topic = LDA.components_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"To sort the indexes according to probability values, we can use the argsort() function. Once sorted, the 10 words with the highest probabilities will now belong to the last 10 indexes of the array. The following script returns the indexes of the 10 words with the highest probabilities:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topic_words = first_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      "india\n",
      "country\n",
      "good\n",
      "state\n",
      "news\n",
      "china\n",
      "pm\n",
      "covid\n",
      "https\n"
     ]
    }
   ],
   "source": [
    "for i in top_topic_words:\n",
    "    print(count_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['going', 'india', 'country', 'good', 'state', 'news', 'china', 'pm', 'covid', 'https']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['president', 'scientist', 'speed', 'us_fda', 'credit', 'biden', 'did', 'wa', 'realdonaldtrump', 'trump']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['ha', 'death', 'immunity', 'need', 'rate', 'virus', 'risk', 'https', 'covid', 'people']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['people', 'world', 'polio', 'just', 'gate', 'year', 'wa', 'make', 'like', 'https']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['plan', 'news', 'pfizer', 'ha', 'distribution', 'trump', 'pandemic', 'need', 'covid', 'https']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['know', 'need', 'going', 'think', 'like', 'don', 'want', 'mask', 'just', 'people']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['year', 'trudeau', 'canadian', 'ha', 'getting', 'canada', 'shot', 'covid', 'https', 'flu']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['mrna', 'year', 'covid', 'term', 'know', 'virus', 'ha', 'long', 'effect', 'trial']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['effective', 'dos', 'emergency', 'approval', 'coronavirus', 'fda', 'pfizer', 'moderna', 'covid', 'https']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['fauci', 'distribution', 'vaccination', 'expert', 'dr', 'coronavirus', 'health', 'say', 'covid', 'https']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179672, 10)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values = LDA.transform(doc_term_matrix_2)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LDA_topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parsed_created_at</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>positive</th>\n",
       "      <th>very_positive</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>pos_tagged</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>final_docs</th>\n",
       "      <th>Topic</th>\n",
       "      <th>LDA Topic</th>\n",
       "      <th>NMF Topic</th>\n",
       "      <th>LDA_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1333608117242720256</td>\n",
       "      <td>2020-12-01 03:05:27+00:00</td>\n",
       "      <td>SpeakerMentors</td>\n",
       "      <td>@Hobie_SHRED I don't think so...this is the fi...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[@hobie_shred, think, so, ..., this, is, the, ...</td>\n",
       "      <td>[(@hobie_shred, VBN), (think, VBP), (so, RB), ...</td>\n",
       "      <td>[@hobie_shred, think, so, ..., this, is, the, ...</td>\n",
       "      <td>@hobie_shred think so ... this is the first ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1333608115816828928</td>\n",
       "      <td>2020-12-01 03:05:26+00:00</td>\n",
       "      <td>JoyceWhyfor</td>\n",
       "      <td>@DrMorien1 i voted for trump but he keeps talk...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[@drmorien, voted, for, trump, but, he, keeps,...</td>\n",
       "      <td>[(@drmorien, NN), (voted, VBD), (for, IN), (tr...</td>\n",
       "      <td>[@drmorien, voted, for, trump, but, he, keep, ...</td>\n",
       "      <td>@drmorien voted for trump but he keep talking ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1333608112314576897</td>\n",
       "      <td>2020-12-01 03:05:25+00:00</td>\n",
       "      <td>BrianCCox2</td>\n",
       "      <td>@ighaworth Thank goodness the Harris-Biden duo...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[@ighaworth, thank, goodness, the, harris-bide...</td>\n",
       "      <td>[(@ighaworth, JJ), (thank, NN), (goodness, NN)...</td>\n",
       "      <td>[@ighaworth, thank, goodness, the, harris-bide...</td>\n",
       "      <td>@ighaworth thank goodness the harris-biden duo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1333608107751170048</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>TheHops31</td>\n",
       "      <td>Covid-19 vaccine: Moderna applies for FDA auth...</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[covid, vaccine, moderna, applies, for, fda, a...</td>\n",
       "      <td>[(covid, JJ), (vaccine, NN), (moderna, NN), (a...</td>\n",
       "      <td>[covid, vaccine, moderna, applies, for, fda, a...</td>\n",
       "      <td>covid vaccine moderna applies for fda authoriz...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1333608105515438080</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>JuCamarote</td>\n",
       "      <td>@LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[@lusyluck, @bleedincubblue, @liliana, @drlean...</td>\n",
       "      <td>[(@lusyluck, JJ), (@bleedincubblue, NNP), (@li...</td>\n",
       "      <td>[@lusyluck, @bleedincubblue, @liliana, @drlean...</td>\n",
       "      <td>@lusyluck @bleedincubblue @liliana @drleanawen...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id          parsed_created_at user_screen_name  \\\n",
       "0  1333608117242720256  2020-12-01 03:05:27+00:00   SpeakerMentors   \n",
       "1  1333608115816828928  2020-12-01 03:05:26+00:00      JoyceWhyfor   \n",
       "2  1333608112314576897  2020-12-01 03:05:25+00:00       BrianCCox2   \n",
       "3  1333608107751170048  2020-12-01 03:05:24+00:00        TheHops31   \n",
       "4  1333608105515438080  2020-12-01 03:05:24+00:00       JuCamarote   \n",
       "\n",
       "                                                text tweet_type hashtags  \\\n",
       "0  @Hobie_SHRED I don't think so...this is the fi...      reply      NaN   \n",
       "1  @DrMorien1 i voted for trump but he keeps talk...      reply      NaN   \n",
       "2  @ighaworth Thank goodness the Harris-Biden duo...      reply      NaN   \n",
       "3  Covid-19 vaccine: Moderna applies for FDA auth...   original      NaN   \n",
       "4  @LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...      reply      NaN   \n",
       "\n",
       "   favorite_count lang possibly_sensitive  retweet_count  ...  positive  \\\n",
       "0               0   en                NaN              0  ...     False   \n",
       "1               0   en                NaN              0  ...     False   \n",
       "2               0   en                NaN              0  ...     False   \n",
       "3               0   en               True              0  ...     False   \n",
       "4               0   en                NaN              0  ...     False   \n",
       "\n",
       "  very_positive                                     processed_text  \\\n",
       "0         False  [@hobie_shred, think, so, ..., this, is, the, ...   \n",
       "1         False  [@drmorien, voted, for, trump, but, he, keeps,...   \n",
       "2          True  [@ighaworth, thank, goodness, the, harris-bide...   \n",
       "3         False  [covid, vaccine, moderna, applies, for, fda, a...   \n",
       "4          True  [@lusyluck, @bleedincubblue, @liliana, @drlean...   \n",
       "\n",
       "                                          pos_tagged  \\\n",
       "0  [(@hobie_shred, VBN), (think, VBP), (so, RB), ...   \n",
       "1  [(@drmorien, NN), (voted, VBD), (for, IN), (tr...   \n",
       "2  [(@ighaworth, JJ), (thank, NN), (goodness, NN)...   \n",
       "3  [(covid, JJ), (vaccine, NN), (moderna, NN), (a...   \n",
       "4  [(@lusyluck, JJ), (@bleedincubblue, NNP), (@li...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [@hobie_shred, think, so, ..., this, is, the, ...   \n",
       "1  [@drmorien, voted, for, trump, but, he, keep, ...   \n",
       "2  [@ighaworth, thank, goodness, the, harris-bide...   \n",
       "3  [covid, vaccine, moderna, applies, for, fda, a...   \n",
       "4  [@lusyluck, @bleedincubblue, @liliana, @drlean...   \n",
       "\n",
       "                                          final_docs  Topic  LDA Topic  \\\n",
       "0  @hobie_shred think so ... this is the first ti...      1          3   \n",
       "1  @drmorien voted for trump but he keep talking ...      1          1   \n",
       "2  @ighaworth thank goodness the harris-biden duo...      1          1   \n",
       "3  covid vaccine moderna applies for fda authoriz...      0          8   \n",
       "4  @lusyluck @bleedincubblue @liliana @drleanawen...      2          7   \n",
       "\n",
       "  NMF Topic LDA_topic  \n",
       "0         1         1  \n",
       "1         4         4  \n",
       "2         4         4  \n",
       "3         5         5  \n",
       "4         1         1  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Topic modeling, using NMF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(n_components=10, random_state=42)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=10, random_state=42)\n",
    "nmf.fit(doc_term_matrix_1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rich\n",
      "effect\n",
      "think vaccine\n",
      "took\n",
      "confirmed\n",
      "evil\n",
      "billion\n",
      "return\n",
      "mollyjongfast\n",
      "cause\n"
     ]
    }
   ],
   "source": [
    "#randomly get ten words from our vocabulary \n",
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(tfidfconverter.get_feature_names()))\n",
    "    print(tfidfconverter.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_topic = nmf.components_[0]\n",
    "top_topic_words = first_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indexes can now be passed to the tfidfconverter object to retrieve the actual words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution\n",
      "trial\n",
      "health\n",
      "say\n",
      "pandemic\n",
      "read\n",
      "vaccines\n",
      "pm\n",
      "news\n",
      "https\n"
     ]
    }
   ],
   "source": [
    "for i in top_topic_words:\n",
    "    print(tfidfconverter.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the ten words with highest probabilities for each of the topics:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['distribution', 'trial', 'health', 'say', 'pandemic', 'read', 'vaccines', 'pm', 'news', 'https']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['make', 'going', 'think', 'want', 'wa', 'know', 'like', 'virus', 'ha', 'just']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['distribution', 'getting covid', 'covidvaccine', 'pfizer', 'available', 'say', 'pfizer covid', 'vaccine covid', 'covid vaccine', 'covid']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['vp', 'asap', 'don need', 'covid pandemic', 'vaccine covid', 'pfizer', 'pandemic', 'vaccine need', 'need vaccine', 'need']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['president', 'did', 'came vaccine', 'came', 'credit', 'biden', 'realdonaldtrump us_fda', 'us_fda', 'trump', 'realdonaldtrump']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['pfizer', 'emergency use', 'moderna vaccine', 'use', 'effective', 'authorization', 'approval', 'fda', 'emergency', 'moderna']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['cdc', 'administration', 'trump administration', 'airline', 'moderna ask', 'dos moderna', 'want', 'distribute', 'https', 'vaccine https']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['like flu', 'swine flu', 'swine', 'vaccine flu', 'vaccine year', 'flu shot', 'shot', 'year', 'flu vaccine', 'flu']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['week', 'report', 'pfizer coronavirus', 'ha', 'health', 'dos', 'say', 'american', 'coronavirus vaccine', 'coronavirus']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['million people', 'don', 'vaccinated', 'die', 'million', 'want', 'think', 'vaccine people', 'people vaccine', 'people']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidfconverter.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial \"human\" analysis of topics detected using NMF: \n",
    "\n",
    "\n",
    "Top 10 words for topic #0:\n",
    "['distribution', 'trial', 'health', 'say', 'pandemic', 'read', 'vaccines', 'pm', 'news', 'https']\n",
    "TOPIC 0: generic discussion of news RE vaccine distribution \n",
    "\n",
    "\n",
    "Top 10 words for topic #1:\n",
    "['make', 'going', 'think', 'want', 'wa', 'know', 'like', 'virus', 'ha', 'just']\n",
    "TOPIC 1: generic  \n",
    "\n",
    "\n",
    "\n",
    "Top 10 words for topic #2:\n",
    "['distribution', 'getting covid', 'covidvaccine', 'pfizer', 'available', 'say', 'pfizer covid', 'vaccine covid', 'covid vaccine', 'covid']\n",
    "TOPIC 2: focused discussion of Pfizer covid vaccine  \n",
    "\n",
    "\n",
    "Top 10 words for topic #3:\n",
    "['vp', 'asap', 'don need', 'covid pandemic', 'vaccine covid', 'pfizer', 'pandemic', 'vaccine need', 'need vaccine', 'need']\n",
    "TOPIC 3: focused discussion of NEED regarding Pfizer covid vaccine  \n",
    "* examine tweets to see what NEED is referring to... \n",
    "\n",
    "\n",
    "Top 10 words for topic #4:\n",
    "['president', 'did', 'came vaccine', 'came', 'credit', 'biden', 'realdonaldtrump us_fda', 'us_fda', 'trump', 'realdonaldtrump']\n",
    "TOPIC 4: U.S. political discussion regarding the vaccine, specifically whether Trump will get credit for speed of vaccine... \n",
    "\n",
    "\n",
    "Top 10 words for topic #5:\n",
    "['pfizer', 'emergency use', 'moderna vaccine', 'use', 'effective', 'authorization', 'approval', 'fda', 'emergency', 'moderna']\n",
    "TOPIC 5: focused discussion of Moderna covid vaccine \n",
    "\n",
    "\n",
    "Top 10 words for topic #6:\n",
    "['cdc', 'administration', 'trump administration', 'airline', 'moderna ask', 'dos moderna', 'want', 'distribute', 'https', 'vaccine https']\n",
    "TOPIC 6: general U.S. discussion of covid vaccine, regarding politics, business, life (ranging from distribution of the vaccine to whether people will need a vaccine for air travel, etc.) \n",
    "\n",
    "\n",
    "Top 10 words for topic #7:\n",
    "['like flu', 'swine flu', 'swine', 'vaccine flu', 'vaccine year', 'flu shot', 'shot', 'year', 'flu vaccine', 'flu']\n",
    "TOPIC 7: discussion of the covid vaccine in relation to flu vaccine\n",
    "* Very interesting topic - look at tweets to see if they seem to be politically-slanted or more of a scientific tone (are people downplaying the need for covid vacine, saying it's no more necessary than a flue vaccine - or are they informing themselves or others about the flu vaccine, history of flu vaccine and history of swine flu, as part of broader discussion of covid? Or something else entirely?) \n",
    "\n",
    "\n",
    "Top 10 words for topic #8:\n",
    "['week', 'report', 'pfizer coronavirus', 'ha', 'health', 'dos', 'say', 'american', 'coronavirus vaccine', 'coronavirus']\n",
    "TOPIC 8: focused discussion of Pfizer vaccine \n",
    "\n",
    "\n",
    "Top 10 words for topic #9:\n",
    "['million people', 'don', 'vaccinated', 'die', 'million', 'want', 'think', 'vaccine people', 'people vaccine', 'people']\n",
    "TOPIC 8: this topic is dominated by extremely strong words and expressions - \"want\", \"die\", \"million.\" ALso, \"people\" is listed several times. Fascinating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script adds the topics to the data set and displays the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parsed_created_at</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>very_positive</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>pos_tagged</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>final_docs</th>\n",
       "      <th>Topic</th>\n",
       "      <th>LDA Topic</th>\n",
       "      <th>NMF Topic</th>\n",
       "      <th>LDA_topic</th>\n",
       "      <th>NMF_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1333608117242720256</td>\n",
       "      <td>2020-12-01 03:05:27+00:00</td>\n",
       "      <td>SpeakerMentors</td>\n",
       "      <td>@Hobie_SHRED I don't think so...this is the fi...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[@hobie_shred, think, so, ..., this, is, the, ...</td>\n",
       "      <td>[(@hobie_shred, VBN), (think, VBP), (so, RB), ...</td>\n",
       "      <td>[@hobie_shred, think, so, ..., this, is, the, ...</td>\n",
       "      <td>@hobie_shred think so ... this is the first ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1333608115816828928</td>\n",
       "      <td>2020-12-01 03:05:26+00:00</td>\n",
       "      <td>JoyceWhyfor</td>\n",
       "      <td>@DrMorien1 i voted for trump but he keeps talk...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[@drmorien, voted, for, trump, but, he, keeps,...</td>\n",
       "      <td>[(@drmorien, NN), (voted, VBD), (for, IN), (tr...</td>\n",
       "      <td>[@drmorien, voted, for, trump, but, he, keep, ...</td>\n",
       "      <td>@drmorien voted for trump but he keep talking ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1333608112314576897</td>\n",
       "      <td>2020-12-01 03:05:25+00:00</td>\n",
       "      <td>BrianCCox2</td>\n",
       "      <td>@ighaworth Thank goodness the Harris-Biden duo...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>[@ighaworth, thank, goodness, the, harris-bide...</td>\n",
       "      <td>[(@ighaworth, JJ), (thank, NN), (goodness, NN)...</td>\n",
       "      <td>[@ighaworth, thank, goodness, the, harris-bide...</td>\n",
       "      <td>@ighaworth thank goodness the harris-biden duo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1333608107751170048</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>TheHops31</td>\n",
       "      <td>Covid-19 vaccine: Moderna applies for FDA auth...</td>\n",
       "      <td>original</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[covid, vaccine, moderna, applies, for, fda, a...</td>\n",
       "      <td>[(covid, JJ), (vaccine, NN), (moderna, NN), (a...</td>\n",
       "      <td>[covid, vaccine, moderna, applies, for, fda, a...</td>\n",
       "      <td>covid vaccine moderna applies for fda authoriz...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1333608105515438080</td>\n",
       "      <td>2020-12-01 03:05:24+00:00</td>\n",
       "      <td>JuCamarote</td>\n",
       "      <td>@LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...</td>\n",
       "      <td>reply</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>[@lusyluck, @bleedincubblue, @liliana, @drlean...</td>\n",
       "      <td>[(@lusyluck, JJ), (@bleedincubblue, NNP), (@li...</td>\n",
       "      <td>[@lusyluck, @bleedincubblue, @liliana, @drlean...</td>\n",
       "      <td>@lusyluck @bleedincubblue @liliana @drleanawen...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id          parsed_created_at user_screen_name  \\\n",
       "0  1333608117242720256  2020-12-01 03:05:27+00:00   SpeakerMentors   \n",
       "1  1333608115816828928  2020-12-01 03:05:26+00:00      JoyceWhyfor   \n",
       "2  1333608112314576897  2020-12-01 03:05:25+00:00       BrianCCox2   \n",
       "3  1333608107751170048  2020-12-01 03:05:24+00:00        TheHops31   \n",
       "4  1333608105515438080  2020-12-01 03:05:24+00:00       JuCamarote   \n",
       "\n",
       "                                                text tweet_type hashtags  \\\n",
       "0  @Hobie_SHRED I don't think so...this is the fi...      reply      NaN   \n",
       "1  @DrMorien1 i voted for trump but he keeps talk...      reply      NaN   \n",
       "2  @ighaworth Thank goodness the Harris-Biden duo...      reply      NaN   \n",
       "3  Covid-19 vaccine: Moderna applies for FDA auth...   original      NaN   \n",
       "4  @LusyLuck @bleedinCubBlue @Liliana22207796 @Dr...      reply      NaN   \n",
       "\n",
       "   favorite_count lang possibly_sensitive  retweet_count  ...  very_positive  \\\n",
       "0               0   en                NaN              0  ...          False   \n",
       "1               0   en                NaN              0  ...          False   \n",
       "2               0   en                NaN              0  ...           True   \n",
       "3               0   en               True              0  ...          False   \n",
       "4               0   en                NaN              0  ...           True   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  [@hobie_shred, think, so, ..., this, is, the, ...   \n",
       "1  [@drmorien, voted, for, trump, but, he, keeps,...   \n",
       "2  [@ighaworth, thank, goodness, the, harris-bide...   \n",
       "3  [covid, vaccine, moderna, applies, for, fda, a...   \n",
       "4  [@lusyluck, @bleedincubblue, @liliana, @drlean...   \n",
       "\n",
       "                                          pos_tagged  \\\n",
       "0  [(@hobie_shred, VBN), (think, VBP), (so, RB), ...   \n",
       "1  [(@drmorien, NN), (voted, VBD), (for, IN), (tr...   \n",
       "2  [(@ighaworth, JJ), (thank, NN), (goodness, NN)...   \n",
       "3  [(covid, JJ), (vaccine, NN), (moderna, NN), (a...   \n",
       "4  [(@lusyluck, JJ), (@bleedincubblue, NNP), (@li...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [@hobie_shred, think, so, ..., this, is, the, ...   \n",
       "1  [@drmorien, voted, for, trump, but, he, keep, ...   \n",
       "2  [@ighaworth, thank, goodness, the, harris-bide...   \n",
       "3  [covid, vaccine, moderna, applies, for, fda, a...   \n",
       "4  [@lusyluck, @bleedincubblue, @liliana, @drlean...   \n",
       "\n",
       "                                          final_docs  Topic  LDA Topic  \\\n",
       "0  @hobie_shred think so ... this is the first ti...      1          3   \n",
       "1  @drmorien voted for trump but he keep talking ...      1          1   \n",
       "2  @ighaworth thank goodness the harris-biden duo...      1          1   \n",
       "3  covid vaccine moderna applies for fda authoriz...      0          8   \n",
       "4  @lusyluck @bleedincubblue @liliana @drleanawen...      2          7   \n",
       "\n",
       "   NMF Topic LDA_topic NMF_topic  \n",
       "0          1         1         1  \n",
       "1          4         4         4  \n",
       "2          4         4         4  \n",
       "3          5         5         5  \n",
       "4          1         1         1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values = nmf.transform(doc_term_matrix_1)\n",
    "df['NMF_topic'] = topic_values.argmax(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Initial analysis of topic modeling results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial look at topic breakdowns suggests that most of the topics make sense: tweets seem to have been divided up into logical categories (ones about vaccines themselves, ones about politics, ones about the vaccine brands, etc.)\n",
    "\n",
    "The NMF breakdown of topics looks much more precise to me, so I'll focus further analysis on those topics. \n",
    "\n",
    "Next step: count how many tweets fit into each class for NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    20461\n",
       "parsed_created_at     20461\n",
       "user_screen_name      20461\n",
       "text                  20461\n",
       "tweet_type            20461\n",
       "hashtags               3992\n",
       "favorite_count        20461\n",
       "lang                  20461\n",
       "possibly_sensitive    19361\n",
       "retweet_count         20461\n",
       "user_id               20461\n",
       "user_location         15075\n",
       "sentiment_score       20461\n",
       "very_negative         20461\n",
       "negative              20461\n",
       "neutral               20461\n",
       "positive              20461\n",
       "very_positive         20461\n",
       "processed_text        20461\n",
       "pos_tagged            20461\n",
       "lemmatized            20461\n",
       "final_docs            20461\n",
       "Topic                 20461\n",
       "LDA Topic             20461\n",
       "NMF Topic             20461\n",
       "LDA_topic             20461\n",
       "NMF_topic             20461\n",
       "dtype: int64"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get NMF counts\n",
    "df[df.NMF_topic == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    64518\n",
       "parsed_created_at     64518\n",
       "user_screen_name      64518\n",
       "text                  64518\n",
       "tweet_type            64518\n",
       "hashtags               4190\n",
       "favorite_count        64518\n",
       "lang                  64518\n",
       "possibly_sensitive     6619\n",
       "retweet_count         64518\n",
       "user_id               64518\n",
       "user_location         41218\n",
       "sentiment_score       64518\n",
       "very_negative         64518\n",
       "negative              64518\n",
       "neutral               64518\n",
       "positive              64518\n",
       "very_positive         64518\n",
       "processed_text        64518\n",
       "pos_tagged            64518\n",
       "lemmatized            64518\n",
       "final_docs            64518\n",
       "Topic                 64518\n",
       "LDA Topic             64518\n",
       "NMF Topic             64518\n",
       "LDA_topic             64518\n",
       "NMF_topic             64518\n",
       "dtype: int64"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get NMF counts\n",
    "df[df.NMF_topic == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    25513\n",
       "parsed_created_at     25513\n",
       "user_screen_name      25513\n",
       "text                  25513\n",
       "tweet_type            25513\n",
       "hashtags               6380\n",
       "favorite_count        25513\n",
       "lang                  25513\n",
       "possibly_sensitive    14625\n",
       "retweet_count         25513\n",
       "user_id               25513\n",
       "user_location         18903\n",
       "sentiment_score       25513\n",
       "very_negative         25513\n",
       "negative              25513\n",
       "neutral               25513\n",
       "positive              25513\n",
       "very_positive         25513\n",
       "processed_text        25513\n",
       "pos_tagged            25513\n",
       "lemmatized            25513\n",
       "final_docs            25513\n",
       "Topic                 25513\n",
       "LDA Topic             25513\n",
       "NMF Topic             25513\n",
       "LDA_topic             25513\n",
       "NMF_topic             25513\n",
       "dtype: int64"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get NMF counts\n",
    "df[df.NMF_topic == 2].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    8148\n",
       "parsed_created_at     8148\n",
       "user_screen_name      8148\n",
       "text                  8148\n",
       "tweet_type            8148\n",
       "hashtags               730\n",
       "favorite_count        8148\n",
       "lang                  8148\n",
       "possibly_sensitive    2435\n",
       "retweet_count         8148\n",
       "user_id               8148\n",
       "user_location         5404\n",
       "sentiment_score       8148\n",
       "very_negative         8148\n",
       "negative              8148\n",
       "neutral               8148\n",
       "positive              8148\n",
       "very_positive         8148\n",
       "processed_text        8148\n",
       "pos_tagged            8148\n",
       "lemmatized            8148\n",
       "final_docs            8148\n",
       "Topic                 8148\n",
       "LDA Topic             8148\n",
       "NMF Topic             8148\n",
       "LDA_topic             8148\n",
       "NMF_topic             8148\n",
       "dtype: int64"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get NMF counts\n",
    "df[df.NMF_topic == 3].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    15253\n",
       "parsed_created_at     15253\n",
       "user_screen_name      15253\n",
       "text                  15253\n",
       "tweet_type            15253\n",
       "hashtags               1365\n",
       "favorite_count        15253\n",
       "lang                  15253\n",
       "possibly_sensitive     2294\n",
       "retweet_count         15253\n",
       "user_id               15253\n",
       "user_location          8076\n",
       "sentiment_score       15253\n",
       "very_negative         15253\n",
       "negative              15253\n",
       "neutral               15253\n",
       "positive              15253\n",
       "very_positive         15253\n",
       "processed_text        15253\n",
       "pos_tagged            15253\n",
       "lemmatized            15253\n",
       "final_docs            15253\n",
       "Topic                 15253\n",
       "LDA Topic             15253\n",
       "NMF Topic             15253\n",
       "LDA_topic             15253\n",
       "NMF_topic             15253\n",
       "dtype: int64"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get NMF counts\n",
    "df[df.NMF_topic == 4].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    7040\n",
       "parsed_created_at     7040\n",
       "user_screen_name      7040\n",
       "text                  7040\n",
       "tweet_type            7040\n",
       "hashtags              1006\n",
       "favorite_count        7040\n",
       "lang                  7040\n",
       "possibly_sensitive    7032\n",
       "retweet_count         7040\n",
       "user_id               7040\n",
       "user_location         5182\n",
       "sentiment_score       7040\n",
       "very_negative         7040\n",
       "negative              7040\n",
       "neutral               7040\n",
       "positive              7040\n",
       "very_positive         7040\n",
       "processed_text        7040\n",
       "pos_tagged            7040\n",
       "lemmatized            7040\n",
       "final_docs            7040\n",
       "Topic                 7040\n",
       "LDA Topic             7040\n",
       "NMF Topic             7040\n",
       "LDA_topic             7040\n",
       "NMF_topic             7040\n",
       "dtype: int64"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get NMF counts\n",
    "df[df.NMF_topic == 6].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    6638\n",
       "parsed_created_at     6638\n",
       "user_screen_name      6638\n",
       "text                  6638\n",
       "tweet_type            6638\n",
       "hashtags               592\n",
       "favorite_count        6638\n",
       "lang                  6638\n",
       "possibly_sensitive    1377\n",
       "retweet_count         6638\n",
       "user_id               6638\n",
       "user_location         4385\n",
       "sentiment_score       6638\n",
       "very_negative         6638\n",
       "negative              6638\n",
       "neutral               6638\n",
       "positive              6638\n",
       "very_positive         6638\n",
       "processed_text        6638\n",
       "pos_tagged            6638\n",
       "lemmatized            6638\n",
       "final_docs            6638\n",
       "Topic                 6638\n",
       "LDA Topic             6638\n",
       "NMF Topic             6638\n",
       "LDA_topic             6638\n",
       "NMF_topic             6638\n",
       "dtype: int64"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get NMF counts\n",
    "df[df.NMF_topic == 7].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    6543\n",
       "parsed_created_at     6543\n",
       "user_screen_name      6543\n",
       "text                  6543\n",
       "tweet_type            6543\n",
       "hashtags              1730\n",
       "favorite_count        6543\n",
       "lang                  6543\n",
       "possibly_sensitive    4805\n",
       "retweet_count         6543\n",
       "user_id               6543\n",
       "user_location         4868\n",
       "sentiment_score       6543\n",
       "very_negative         6543\n",
       "negative              6543\n",
       "neutral               6543\n",
       "positive              6543\n",
       "very_positive         6543\n",
       "processed_text        6543\n",
       "pos_tagged            6543\n",
       "lemmatized            6543\n",
       "final_docs            6543\n",
       "Topic                 6543\n",
       "LDA Topic             6543\n",
       "NMF Topic             6543\n",
       "LDA_topic             6543\n",
       "NMF_topic             6543\n",
       "dtype: int64"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get NMF counts\n",
    "df[df.NMF_topic == 8].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    15214\n",
       "parsed_created_at     15214\n",
       "user_screen_name      15214\n",
       "text                  15214\n",
       "tweet_type            15214\n",
       "hashtags               1096\n",
       "favorite_count        15214\n",
       "lang                  15214\n",
       "possibly_sensitive     3006\n",
       "retweet_count         15214\n",
       "user_id               15214\n",
       "user_location          9663\n",
       "sentiment_score       15214\n",
       "very_negative         15214\n",
       "negative              15214\n",
       "neutral               15214\n",
       "positive              15214\n",
       "very_positive         15214\n",
       "processed_text        15214\n",
       "pos_tagged            15214\n",
       "lemmatized            15214\n",
       "final_docs            15214\n",
       "Topic                 15214\n",
       "LDA Topic             15214\n",
       "NMF Topic             15214\n",
       "LDA_topic             15214\n",
       "NMF_topic             15214\n",
       "dtype: int64"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get NMF counts\n",
    "df[df.NMF_topic == 9].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[df.NMF_topic == 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15214, 27)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.NMF_topic == 9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topic9_tweets = df[df.NMF_topic == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nmf_topic9_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "382    @DonaldJTrumpJr The Governor’s in every state are the ones who let all these people die, New York, Cuomo, Cali, Newsome. The OBiden admin sourced out production of Med equipment to China. Trump commissioned AMERICAN Companies to build and make a vaccine. I’d say that’s a dam good job                         \n",
       "388    @WeHave2BeBetter Is how i see it. So far I heard most people in study had no side effects. I belive this will be Moderna's vaccine which requires vaccine to stay in regular refrigerator temp for up to 3 days after being removed from freezer.                                                                    \n",
       "401    @mtmalinen Got it. Instead you'll risk your health to a new disease whose long-term effects are unknown. But which is known to very often have serious medium-term effects even in younger people. While the vaccines are known to have none. Smart move.                                                            \n",
       "424    @bkkid19 @NYGovCuomo I hope people like you are first in line to take the vaccine! \\nGood riddens!                                                                                                                                                                                                                   \n",
       "430    @BitterCaptain I’m fine with them getting an experimental vaccine.If it turns them infertile they won’t reproduce and we don’t need more stupid people responsible 4 closure, if it gives them man titty they already had it and if it paralyzes them it doesn’t change much of their lifestyle anyways              \n",
       "447    @youngireland16 It will be much harder for people who are dependent on the system for survival, to refuse the vaccine. Easier for people who are more independent and self reliant.                                                                                                                                  \n",
       "449    @Reetzy830 @KamalaHarris They cant force you to get it. Vaccines are designed so that when able bodied people get them, herd immunity protects those who can’t. 1st priority includes the elderly so it has to be safe for a very vulnerable population. The biggest danger is fear mongering and misinformation     \n",
       "496    @dereksstratton The elderly almost universally have lower rates of adverse reaction to vaccines, because their immune system is less responsive. I am worried about younger people though, yes. I recommend younger people go with something other than an RNA vaccine. See my tweets about Novavax.                 \n",
       "497    An absolutely ridiculous and unethical question. Everyone in (forced) congregate settings, whether prisons or nursing homes, should receive a vaccine first. Congregate settings lead to uncontrolled outbreaks. Incarcerated people don't \"deserve\" COVID more than anyone else. https://t.co/ncpsyd81y4            \n",
       "504    @PhillyStars27 @AndrewPerpetua Or there are people like me that have a strong immune system and know the stats for healthy people my age. I'm not anti vax but I don't trust a rushed vaccine for a virus that is very unlikely to effect me. Plus, if even 60% get the vaccine that likely takes us to herd immunity\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "nmf_topic9_tweets[20:30]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "830     Y’all out here eating corn fed cow and swine and worried about what’s in a vaccine                                                                                                                                                                                                                                                           \n",
       "871     @MattWalshBlog We have a flu vaccine.                                                                                                                                                                                                                                                                                                        \n",
       "927     @Averysdaddy84 And it’s new so it took time to figure out effective treatments and develop a vaccine. We have decades of experience with the flu. If we had been giving a clear direct message from our leaders we may have been closer to being on the same page. However, that was not the case                                            \n",
       "1058    @Me57083635 @JogaBonito888 @FatEmperor I've been pro-vaccine all my life, thank God I'm of an age where I received the TB jab as research has shown this gives some immunity to covid. I've sold the flu vaccine commercially for a Pharma, my background is holistic health. I've had the flu vaccine annually for some 18yrs.              \n",
       "1202    Idk about you guys but I'll probs still wear a mask in public post vaccine because not getting and spreading strep, the flu, etc is really awesome                                                                                                                                                                                           \n",
       "1226    @HokieMcgibblets They gotta do it to squeeze money from the local govts, but there’s no money to squeeze this year. They’ll have to ramp up service after a vaccine and offices reopen                                                                                                                                                       \n",
       "1229    @LawLiberal @KamalaHarris What are you going to do when Covid 19 mutates and the Vaccine doesn't work anymore? Look at the common FLU. The FLU vaccine doesn't keep you from getting the FLU so what makes you think the Covid 19 VACCINE is going to mean you won't get Covid 19                                                            \n",
       "1284    @LusyLuck @bleedinCubBlue @Liliana22207796 @DrLeanaWen Nobody really knows if those will actually be the circulating flu viruses in the next season -  that’s why flu vaccines are not that effective.                                                                                                                                       \n",
       "1356    @LusyLuck @bleedinCubBlue @Liliana22207796 @DrLeanaWen These previous tweets explain why a flu vaccine of 50% or more is considered effective enough to be approved by reg authorities - the scientists identify every year 3 or 4 strains of flu viruses that seem to potentially be at risk of an outbreak and develop a vaccine for those.\n",
       "1385    @basedgoodstone @WordandWitness @YALiberty @NYGovCuomo Elites love vaccines, while the politicians worldwide pretend to get their flu shots every year they push government funded eugenics vaccine programs for population control. People are easier to control, manipulate and steal from when they can’t think straight and are sick.    \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.NMF_topic == 7][20:30]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178072    @williamlegate The Bill Gates vaccine is for poor people but I'll be able to listen to podcast in my own head.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.NMF_topic == 9][14004:14005]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38539    @stacy_rumpf @voxdotcom The health care workers in my family don’t want the vaccine, not for them, or their children. Healthy people don’t need it anyway. It’s not worth the risk. It’s new and untested\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.NMF_topic == 9][3333:3334]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65985    @donwinslow People just can't put things aside for 1 year so they and others can stay healthy while a vaccine is developed and distributed. Selfish idiots.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.NMF_topic == 9][5555:5556]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126392    on top of that, the vaccine won't just automatically be an instant hit. in fact, reports of some pretty shitty side effects (even mild cases of covid) are gonna prevent people from following through with the full vaccination\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.NMF_topic == 9][10000:10001]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26313    @MichealMartinTD A lot of people I spoke to say they won't be taking the vaccine. Lots of people are worried about serious adverse effects. I hear the government has indemnified the vaccine manufacturers against any public liability.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.NMF_topic == 9][2222:2223]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
